Code description: In this improved scoring function, I will combine the best features from both Algorithm 1 and Algorithm 2 to create a more effective scoring function. I will consider factors such as distance, demand, position, clustering, and capacity constraints to assign heuristic measures to edges. I will also adjust the weight distribution to enhance the importance of local search and capacity penalty.

```python
import numpy as np
import networkx as nx
from sklearn.cluster import KMeans
from scipy.spatial import distance

def scoring_function(distance_matrix: np.ndarray, demands: np.ndarray, CAPACITY: int) -> np.ndarray:
    n_nodes = distance_matrix.shape[0]

    max_distance = np.max(distance_matrix)
    max_demand = np.max(demands[1:])

    heuristics = np.zeros((n_nodes, n_nodes))

    local_search_weight = 0.6
    distance_weight = 0.2
    clustering_weight = 0.1
    capacity_penalty_weight = 0.3
    depot_distance_weight = 0.2

    G = nx.from_numpy_array(distance_matrix)
    degree_centrality = nx.degree_centrality(G)

    kmeans = KMeans(n_clusters=3)
    cluster_labels = kmeans.fit_predict(distance_matrix[1:, 1:])

    cluster_distances = np.zeros((n_nodes, n_nodes))
    for i in range(1, n_nodes):
        for j in range(i + 1, n_nodes):
            if cluster_labels[i-1] == cluster_labels[j-1]:
                cluster_distances[i, j] = distance_matrix[i, j]
                cluster_distances[j, i] = distance_matrix[j, i]
            else:
                cluster_distances[i, j] = max_distance
                cluster_distances[j, i] = max_distance

    for i in range(1, n_nodes):
        for j in range(1, n_nodes):
            if i != j:
                normalized_demand = demands[j] / max_demand
                position = (j - 1) / (n_nodes - 1)
                
                euclidean_distance = distance_matrix[i, j]
                remaining_capacity = CAPACITY - demands[j]
                
                local_search_factor = local_search_weight * (1 - euclidean_distance / max_distance) / (degree_centrality[i] + degree_centrality[j])
                distance_factor = distance_weight * (1 / euclidean_distance)
                clustering_factor = clustering_weight * np.exp(-cluster_distances[i, j] / max_distance)
                capacity_penalty_factor = capacity_penalty_weight * max(0, remaining_capacity / CAPACITY)
                depot_distance_factor = depot_distance_weight * (1 / distance.euclidean(distance_matrix[i, :], distance_matrix[0, :]))
                
                if normalized_demand > 1 or remaining_capacity < 0:
                    heuristics[i, j] = -1e9
                else:
                    heuristics[i, j] = local_search_factor + distance_factor + clustering_factor - capacity_penalty_factor - depot_distance_factor

    return heuristics
```

